# LangSmith (observability) — set true for traces; helps debug multi-agent runs and latency
LANGCHAIN_TRACING_V2=true
# API key from https://smith.langchain.com (or your LangSmith host)
LANGCHAIN_API_KEY=
# Optional: project name in LangSmith for grouping runs (default: default)
# LANGCHAIN_PROJECT=automaton-auditor

# Groq (free tier) — Judges; optional RepoInvestigator summary (when JUDGE_PROVIDER=groq)
GROQ_API_KEY=
# Judge model (default llama-3.3-70b-versatile). If rate limited try llama-3.1-8b-instant
# GROQ_JUDGE_MODEL=llama-3.3-70b-versatile
# Judge provider: ollama (default, local Llama 3.2) | groq | google
# JUDGE_PROVIDER=ollama

# Ollama (local) — Judges and optional RepoInvestigator when JUDGE_PROVIDER=ollama. Run: ollama pull llama3.2
# OLLAMA_MODEL=llama3.2
# OLLAMA_BASE_URL=http://localhost:11434

# Google AI Studio (Gemini) — DocAnalyst, VisionInspector; optional Judges when JUDGE_PROVIDER=google or Groq 429
GOOGLE_API_KEY=
# Optional: Gemini model (default gemini-2.0-flash). If 404 try gemini-1.5-flash-latest or gemini-1.5-flash-002
# GOOGLE_GEMINI_MODEL=gemini-2.0-flash

# Optional override for local testing (default stack does not require OpenAI)
# OPENAI_API_KEY=

# Scalability: parallel workers and rate limit (optional)
# AUDITOR_DETECTIVE_WORKERS=3
# AUDITOR_JUDGE_WORKERS=3
# AUDITOR_MAX_CONCURRENT_RUNS=2
